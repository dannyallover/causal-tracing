{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 27 06:05:35 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    82W / 300W |  18985MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3294293      C   ...x/anaconda3.8/bin/python3    18983MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unseal -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from statistics import stdev\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import unseal\n",
    "from unseal.hooks import Hook, HookedModel, common_hooks\n",
    "from unseal.transformers_util import load_from_pretrained, get_num_layers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../lib\")\n",
    "import utility\n",
    "import hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhooked_model, tokenizer, config = load_from_pretrained('gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and on device cuda!\n"
     ]
    }
   ],
   "source": [
    "model = HookedModel(unhooked_model)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(f'Model loaded and on device {device}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'transformer->wte', 'transformer->wpe', 'transformer->drop', 'transformer->h', 'transformer->h->0', 'transformer->h->0->ln_1', 'transformer->h->0->attn', 'transformer->h->0->attn->c_attn', 'transformer->h->0->attn->c_proj', 'transformer->h->0->attn->attn_dropout', 'transformer->h->0->attn->resid_dropout', 'transformer->h->0->ln_2', 'transformer->h->0->mlp', 'transformer->h->0->mlp->c_fc', 'transformer->h->0->mlp->c_proj', 'transformer->h->0->mlp->act', 'transformer->h->0->mlp->dropout', 'transformer->h->1', 'transformer->h->1->ln_1', 'transformer->h->1->attn', 'transformer->h->1->attn->c_attn', 'transformer->h->1->attn->c_proj', 'transformer->h->1->attn->attn_dropout', 'transformer->h->1->attn->resid_dropout', 'transformer->h->1->ln_2', 'transformer->h->1->mlp', 'transformer->h->1->mlp->c_fc', 'transformer->h->1->mlp->c_proj', 'transformer->h->1->mlp->act', 'transformer->h->1->mlp->dropout', 'transformer->h->2', 'transformer->h->2->ln_1', 'transformer->h->2->attn', 'transformer->h->2->attn->c_attn', 'transformer->h->2->attn->c_proj', 'transformer->h->2->attn->attn_dropout', 'transformer->h->2->attn->resid_dropout', 'transformer->h->2->ln_2', 'transformer->h->2->mlp', 'transformer->h->2->mlp->c_fc', 'transformer->h->2->mlp->c_proj', 'transformer->h->2->mlp->act', 'transformer->h->2->mlp->dropout', 'transformer->h->3', 'transformer->h->3->ln_1', 'transformer->h->3->attn', 'transformer->h->3->attn->c_attn', 'transformer->h->3->attn->c_proj', 'transformer->h->3->attn->attn_dropout', 'transformer->h->3->attn->resid_dropout', 'transformer->h->3->ln_2', 'transformer->h->3->mlp', 'transformer->h->3->mlp->c_fc', 'transformer->h->3->mlp->c_proj', 'transformer->h->3->mlp->act', 'transformer->h->3->mlp->dropout', 'transformer->h->4', 'transformer->h->4->ln_1', 'transformer->h->4->attn', 'transformer->h->4->attn->c_attn', 'transformer->h->4->attn->c_proj', 'transformer->h->4->attn->attn_dropout', 'transformer->h->4->attn->resid_dropout', 'transformer->h->4->ln_2', 'transformer->h->4->mlp', 'transformer->h->4->mlp->c_fc', 'transformer->h->4->mlp->c_proj', 'transformer->h->4->mlp->act', 'transformer->h->4->mlp->dropout', 'transformer->h->5', 'transformer->h->5->ln_1', 'transformer->h->5->attn', 'transformer->h->5->attn->c_attn', 'transformer->h->5->attn->c_proj', 'transformer->h->5->attn->attn_dropout', 'transformer->h->5->attn->resid_dropout', 'transformer->h->5->ln_2', 'transformer->h->5->mlp', 'transformer->h->5->mlp->c_fc', 'transformer->h->5->mlp->c_proj', 'transformer->h->5->mlp->act', 'transformer->h->5->mlp->dropout', 'transformer->h->6', 'transformer->h->6->ln_1', 'transformer->h->6->attn', 'transformer->h->6->attn->c_attn', 'transformer->h->6->attn->c_proj', 'transformer->h->6->attn->attn_dropout', 'transformer->h->6->attn->resid_dropout', 'transformer->h->6->ln_2', 'transformer->h->6->mlp', 'transformer->h->6->mlp->c_fc', 'transformer->h->6->mlp->c_proj', 'transformer->h->6->mlp->act', 'transformer->h->6->mlp->dropout', 'transformer->h->7', 'transformer->h->7->ln_1', 'transformer->h->7->attn', 'transformer->h->7->attn->c_attn', 'transformer->h->7->attn->c_proj', 'transformer->h->7->attn->attn_dropout', 'transformer->h->7->attn->resid_dropout', 'transformer->h->7->ln_2', 'transformer->h->7->mlp', 'transformer->h->7->mlp->c_fc', 'transformer->h->7->mlp->c_proj', 'transformer->h->7->mlp->act', 'transformer->h->7->mlp->dropout', 'transformer->h->8', 'transformer->h->8->ln_1', 'transformer->h->8->attn', 'transformer->h->8->attn->c_attn', 'transformer->h->8->attn->c_proj', 'transformer->h->8->attn->attn_dropout', 'transformer->h->8->attn->resid_dropout', 'transformer->h->8->ln_2', 'transformer->h->8->mlp', 'transformer->h->8->mlp->c_fc', 'transformer->h->8->mlp->c_proj', 'transformer->h->8->mlp->act', 'transformer->h->8->mlp->dropout', 'transformer->h->9', 'transformer->h->9->ln_1', 'transformer->h->9->attn', 'transformer->h->9->attn->c_attn', 'transformer->h->9->attn->c_proj', 'transformer->h->9->attn->attn_dropout', 'transformer->h->9->attn->resid_dropout', 'transformer->h->9->ln_2', 'transformer->h->9->mlp', 'transformer->h->9->mlp->c_fc', 'transformer->h->9->mlp->c_proj', 'transformer->h->9->mlp->act', 'transformer->h->9->mlp->dropout', 'transformer->h->10', 'transformer->h->10->ln_1', 'transformer->h->10->attn', 'transformer->h->10->attn->c_attn', 'transformer->h->10->attn->c_proj', 'transformer->h->10->attn->attn_dropout', 'transformer->h->10->attn->resid_dropout', 'transformer->h->10->ln_2', 'transformer->h->10->mlp', 'transformer->h->10->mlp->c_fc', 'transformer->h->10->mlp->c_proj', 'transformer->h->10->mlp->act', 'transformer->h->10->mlp->dropout', 'transformer->h->11', 'transformer->h->11->ln_1', 'transformer->h->11->attn', 'transformer->h->11->attn->c_attn', 'transformer->h->11->attn->c_proj', 'transformer->h->11->attn->attn_dropout', 'transformer->h->11->attn->resid_dropout', 'transformer->h->11->ln_2', 'transformer->h->11->mlp', 'transformer->h->11->mlp->c_fc', 'transformer->h->11->mlp->c_proj', 'transformer->h->11->mlp->act', 'transformer->h->11->mlp->dropout', 'transformer->h->12', 'transformer->h->12->ln_1', 'transformer->h->12->attn', 'transformer->h->12->attn->c_attn', 'transformer->h->12->attn->c_proj', 'transformer->h->12->attn->attn_dropout', 'transformer->h->12->attn->resid_dropout', 'transformer->h->12->ln_2', 'transformer->h->12->mlp', 'transformer->h->12->mlp->c_fc', 'transformer->h->12->mlp->c_proj', 'transformer->h->12->mlp->act', 'transformer->h->12->mlp->dropout', 'transformer->h->13', 'transformer->h->13->ln_1', 'transformer->h->13->attn', 'transformer->h->13->attn->c_attn', 'transformer->h->13->attn->c_proj', 'transformer->h->13->attn->attn_dropout', 'transformer->h->13->attn->resid_dropout', 'transformer->h->13->ln_2', 'transformer->h->13->mlp', 'transformer->h->13->mlp->c_fc', 'transformer->h->13->mlp->c_proj', 'transformer->h->13->mlp->act', 'transformer->h->13->mlp->dropout', 'transformer->h->14', 'transformer->h->14->ln_1', 'transformer->h->14->attn', 'transformer->h->14->attn->c_attn', 'transformer->h->14->attn->c_proj', 'transformer->h->14->attn->attn_dropout', 'transformer->h->14->attn->resid_dropout', 'transformer->h->14->ln_2', 'transformer->h->14->mlp', 'transformer->h->14->mlp->c_fc', 'transformer->h->14->mlp->c_proj', 'transformer->h->14->mlp->act', 'transformer->h->14->mlp->dropout', 'transformer->h->15', 'transformer->h->15->ln_1', 'transformer->h->15->attn', 'transformer->h->15->attn->c_attn', 'transformer->h->15->attn->c_proj', 'transformer->h->15->attn->attn_dropout', 'transformer->h->15->attn->resid_dropout', 'transformer->h->15->ln_2', 'transformer->h->15->mlp', 'transformer->h->15->mlp->c_fc', 'transformer->h->15->mlp->c_proj', 'transformer->h->15->mlp->act', 'transformer->h->15->mlp->dropout', 'transformer->h->16', 'transformer->h->16->ln_1', 'transformer->h->16->attn', 'transformer->h->16->attn->c_attn', 'transformer->h->16->attn->c_proj', 'transformer->h->16->attn->attn_dropout', 'transformer->h->16->attn->resid_dropout', 'transformer->h->16->ln_2', 'transformer->h->16->mlp', 'transformer->h->16->mlp->c_fc', 'transformer->h->16->mlp->c_proj', 'transformer->h->16->mlp->act', 'transformer->h->16->mlp->dropout', 'transformer->h->17', 'transformer->h->17->ln_1', 'transformer->h->17->attn', 'transformer->h->17->attn->c_attn', 'transformer->h->17->attn->c_proj', 'transformer->h->17->attn->attn_dropout', 'transformer->h->17->attn->resid_dropout', 'transformer->h->17->ln_2', 'transformer->h->17->mlp', 'transformer->h->17->mlp->c_fc', 'transformer->h->17->mlp->c_proj', 'transformer->h->17->mlp->act', 'transformer->h->17->mlp->dropout', 'transformer->h->18', 'transformer->h->18->ln_1', 'transformer->h->18->attn', 'transformer->h->18->attn->c_attn', 'transformer->h->18->attn->c_proj', 'transformer->h->18->attn->attn_dropout', 'transformer->h->18->attn->resid_dropout', 'transformer->h->18->ln_2', 'transformer->h->18->mlp', 'transformer->h->18->mlp->c_fc', 'transformer->h->18->mlp->c_proj', 'transformer->h->18->mlp->act', 'transformer->h->18->mlp->dropout', 'transformer->h->19', 'transformer->h->19->ln_1', 'transformer->h->19->attn', 'transformer->h->19->attn->c_attn', 'transformer->h->19->attn->c_proj', 'transformer->h->19->attn->attn_dropout', 'transformer->h->19->attn->resid_dropout', 'transformer->h->19->ln_2', 'transformer->h->19->mlp', 'transformer->h->19->mlp->c_fc', 'transformer->h->19->mlp->c_proj', 'transformer->h->19->mlp->act', 'transformer->h->19->mlp->dropout', 'transformer->h->20', 'transformer->h->20->ln_1', 'transformer->h->20->attn', 'transformer->h->20->attn->c_attn', 'transformer->h->20->attn->c_proj', 'transformer->h->20->attn->attn_dropout', 'transformer->h->20->attn->resid_dropout', 'transformer->h->20->ln_2', 'transformer->h->20->mlp', 'transformer->h->20->mlp->c_fc', 'transformer->h->20->mlp->c_proj', 'transformer->h->20->mlp->act', 'transformer->h->20->mlp->dropout', 'transformer->h->21', 'transformer->h->21->ln_1', 'transformer->h->21->attn', 'transformer->h->21->attn->c_attn', 'transformer->h->21->attn->c_proj', 'transformer->h->21->attn->attn_dropout', 'transformer->h->21->attn->resid_dropout', 'transformer->h->21->ln_2', 'transformer->h->21->mlp', 'transformer->h->21->mlp->c_fc', 'transformer->h->21->mlp->c_proj', 'transformer->h->21->mlp->act', 'transformer->h->21->mlp->dropout', 'transformer->h->22', 'transformer->h->22->ln_1', 'transformer->h->22->attn', 'transformer->h->22->attn->c_attn', 'transformer->h->22->attn->c_proj', 'transformer->h->22->attn->attn_dropout', 'transformer->h->22->attn->resid_dropout', 'transformer->h->22->ln_2', 'transformer->h->22->mlp', 'transformer->h->22->mlp->c_fc', 'transformer->h->22->mlp->c_proj', 'transformer->h->22->mlp->act', 'transformer->h->22->mlp->dropout', 'transformer->h->23', 'transformer->h->23->ln_1', 'transformer->h->23->attn', 'transformer->h->23->attn->c_attn', 'transformer->h->23->attn->c_proj', 'transformer->h->23->attn->attn_dropout', 'transformer->h->23->attn->resid_dropout', 'transformer->h->23->ln_2', 'transformer->h->23->mlp', 'transformer->h->23->mlp->c_fc', 'transformer->h->23->mlp->c_proj', 'transformer->h->23->mlp->act', 'transformer->h->23->mlp->dropout', 'transformer->h->24', 'transformer->h->24->ln_1', 'transformer->h->24->attn', 'transformer->h->24->attn->c_attn', 'transformer->h->24->attn->c_proj', 'transformer->h->24->attn->attn_dropout', 'transformer->h->24->attn->resid_dropout', 'transformer->h->24->ln_2', 'transformer->h->24->mlp', 'transformer->h->24->mlp->c_fc', 'transformer->h->24->mlp->c_proj', 'transformer->h->24->mlp->act', 'transformer->h->24->mlp->dropout', 'transformer->h->25', 'transformer->h->25->ln_1', 'transformer->h->25->attn', 'transformer->h->25->attn->c_attn', 'transformer->h->25->attn->c_proj', 'transformer->h->25->attn->attn_dropout', 'transformer->h->25->attn->resid_dropout', 'transformer->h->25->ln_2', 'transformer->h->25->mlp', 'transformer->h->25->mlp->c_fc', 'transformer->h->25->mlp->c_proj', 'transformer->h->25->mlp->act', 'transformer->h->25->mlp->dropout', 'transformer->h->26', 'transformer->h->26->ln_1', 'transformer->h->26->attn', 'transformer->h->26->attn->c_attn', 'transformer->h->26->attn->c_proj', 'transformer->h->26->attn->attn_dropout', 'transformer->h->26->attn->resid_dropout', 'transformer->h->26->ln_2', 'transformer->h->26->mlp', 'transformer->h->26->mlp->c_fc', 'transformer->h->26->mlp->c_proj', 'transformer->h->26->mlp->act', 'transformer->h->26->mlp->dropout', 'transformer->h->27', 'transformer->h->27->ln_1', 'transformer->h->27->attn', 'transformer->h->27->attn->c_attn', 'transformer->h->27->attn->c_proj', 'transformer->h->27->attn->attn_dropout', 'transformer->h->27->attn->resid_dropout', 'transformer->h->27->ln_2', 'transformer->h->27->mlp', 'transformer->h->27->mlp->c_fc', 'transformer->h->27->mlp->c_proj', 'transformer->h->27->mlp->act', 'transformer->h->27->mlp->dropout', 'transformer->h->28', 'transformer->h->28->ln_1', 'transformer->h->28->attn', 'transformer->h->28->attn->c_attn', 'transformer->h->28->attn->c_proj', 'transformer->h->28->attn->attn_dropout', 'transformer->h->28->attn->resid_dropout', 'transformer->h->28->ln_2', 'transformer->h->28->mlp', 'transformer->h->28->mlp->c_fc', 'transformer->h->28->mlp->c_proj', 'transformer->h->28->mlp->act', 'transformer->h->28->mlp->dropout', 'transformer->h->29', 'transformer->h->29->ln_1', 'transformer->h->29->attn', 'transformer->h->29->attn->c_attn', 'transformer->h->29->attn->c_proj', 'transformer->h->29->attn->attn_dropout', 'transformer->h->29->attn->resid_dropout', 'transformer->h->29->ln_2', 'transformer->h->29->mlp', 'transformer->h->29->mlp->c_fc', 'transformer->h->29->mlp->c_proj', 'transformer->h->29->mlp->act', 'transformer->h->29->mlp->dropout', 'transformer->h->30', 'transformer->h->30->ln_1', 'transformer->h->30->attn', 'transformer->h->30->attn->c_attn', 'transformer->h->30->attn->c_proj', 'transformer->h->30->attn->attn_dropout', 'transformer->h->30->attn->resid_dropout', 'transformer->h->30->ln_2', 'transformer->h->30->mlp', 'transformer->h->30->mlp->c_fc', 'transformer->h->30->mlp->c_proj', 'transformer->h->30->mlp->act', 'transformer->h->30->mlp->dropout', 'transformer->h->31', 'transformer->h->31->ln_1', 'transformer->h->31->attn', 'transformer->h->31->attn->c_attn', 'transformer->h->31->attn->c_proj', 'transformer->h->31->attn->attn_dropout', 'transformer->h->31->attn->resid_dropout', 'transformer->h->31->ln_2', 'transformer->h->31->mlp', 'transformer->h->31->mlp->c_fc', 'transformer->h->31->mlp->c_proj', 'transformer->h->31->mlp->act', 'transformer->h->31->mlp->dropout', 'transformer->h->32', 'transformer->h->32->ln_1', 'transformer->h->32->attn', 'transformer->h->32->attn->c_attn', 'transformer->h->32->attn->c_proj', 'transformer->h->32->attn->attn_dropout', 'transformer->h->32->attn->resid_dropout', 'transformer->h->32->ln_2', 'transformer->h->32->mlp', 'transformer->h->32->mlp->c_fc', 'transformer->h->32->mlp->c_proj', 'transformer->h->32->mlp->act', 'transformer->h->32->mlp->dropout', 'transformer->h->33', 'transformer->h->33->ln_1', 'transformer->h->33->attn', 'transformer->h->33->attn->c_attn', 'transformer->h->33->attn->c_proj', 'transformer->h->33->attn->attn_dropout', 'transformer->h->33->attn->resid_dropout', 'transformer->h->33->ln_2', 'transformer->h->33->mlp', 'transformer->h->33->mlp->c_fc', 'transformer->h->33->mlp->c_proj', 'transformer->h->33->mlp->act', 'transformer->h->33->mlp->dropout', 'transformer->h->34', 'transformer->h->34->ln_1', 'transformer->h->34->attn', 'transformer->h->34->attn->c_attn', 'transformer->h->34->attn->c_proj', 'transformer->h->34->attn->attn_dropout', 'transformer->h->34->attn->resid_dropout', 'transformer->h->34->ln_2', 'transformer->h->34->mlp', 'transformer->h->34->mlp->c_fc', 'transformer->h->34->mlp->c_proj', 'transformer->h->34->mlp->act', 'transformer->h->34->mlp->dropout', 'transformer->h->35', 'transformer->h->35->ln_1', 'transformer->h->35->attn', 'transformer->h->35->attn->c_attn', 'transformer->h->35->attn->c_proj', 'transformer->h->35->attn->attn_dropout', 'transformer->h->35->attn->resid_dropout', 'transformer->h->35->ln_2', 'transformer->h->35->mlp', 'transformer->h->35->mlp->c_fc', 'transformer->h->35->mlp->c_proj', 'transformer->h->35->mlp->act', 'transformer->h->35->mlp->dropout', 'transformer->h->36', 'transformer->h->36->ln_1', 'transformer->h->36->attn', 'transformer->h->36->attn->c_attn', 'transformer->h->36->attn->c_proj', 'transformer->h->36->attn->attn_dropout', 'transformer->h->36->attn->resid_dropout', 'transformer->h->36->ln_2', 'transformer->h->36->mlp', 'transformer->h->36->mlp->c_fc', 'transformer->h->36->mlp->c_proj', 'transformer->h->36->mlp->act', 'transformer->h->36->mlp->dropout', 'transformer->h->37', 'transformer->h->37->ln_1', 'transformer->h->37->attn', 'transformer->h->37->attn->c_attn', 'transformer->h->37->attn->c_proj', 'transformer->h->37->attn->attn_dropout', 'transformer->h->37->attn->resid_dropout', 'transformer->h->37->ln_2', 'transformer->h->37->mlp', 'transformer->h->37->mlp->c_fc', 'transformer->h->37->mlp->c_proj', 'transformer->h->37->mlp->act', 'transformer->h->37->mlp->dropout', 'transformer->h->38', 'transformer->h->38->ln_1', 'transformer->h->38->attn', 'transformer->h->38->attn->c_attn', 'transformer->h->38->attn->c_proj', 'transformer->h->38->attn->attn_dropout', 'transformer->h->38->attn->resid_dropout', 'transformer->h->38->ln_2', 'transformer->h->38->mlp', 'transformer->h->38->mlp->c_fc', 'transformer->h->38->mlp->c_proj', 'transformer->h->38->mlp->act', 'transformer->h->38->mlp->dropout', 'transformer->h->39', 'transformer->h->39->ln_1', 'transformer->h->39->attn', 'transformer->h->39->attn->c_attn', 'transformer->h->39->attn->c_proj', 'transformer->h->39->attn->attn_dropout', 'transformer->h->39->attn->resid_dropout', 'transformer->h->39->ln_2', 'transformer->h->39->mlp', 'transformer->h->39->mlp->c_fc', 'transformer->h->39->mlp->c_proj', 'transformer->h->39->mlp->act', 'transformer->h->39->mlp->dropout', 'transformer->h->40', 'transformer->h->40->ln_1', 'transformer->h->40->attn', 'transformer->h->40->attn->c_attn', 'transformer->h->40->attn->c_proj', 'transformer->h->40->attn->attn_dropout', 'transformer->h->40->attn->resid_dropout', 'transformer->h->40->ln_2', 'transformer->h->40->mlp', 'transformer->h->40->mlp->c_fc', 'transformer->h->40->mlp->c_proj', 'transformer->h->40->mlp->act', 'transformer->h->40->mlp->dropout', 'transformer->h->41', 'transformer->h->41->ln_1', 'transformer->h->41->attn', 'transformer->h->41->attn->c_attn', 'transformer->h->41->attn->c_proj', 'transformer->h->41->attn->attn_dropout', 'transformer->h->41->attn->resid_dropout', 'transformer->h->41->ln_2', 'transformer->h->41->mlp', 'transformer->h->41->mlp->c_fc', 'transformer->h->41->mlp->c_proj', 'transformer->h->41->mlp->act', 'transformer->h->41->mlp->dropout', 'transformer->h->42', 'transformer->h->42->ln_1', 'transformer->h->42->attn', 'transformer->h->42->attn->c_attn', 'transformer->h->42->attn->c_proj', 'transformer->h->42->attn->attn_dropout', 'transformer->h->42->attn->resid_dropout', 'transformer->h->42->ln_2', 'transformer->h->42->mlp', 'transformer->h->42->mlp->c_fc', 'transformer->h->42->mlp->c_proj', 'transformer->h->42->mlp->act', 'transformer->h->42->mlp->dropout', 'transformer->h->43', 'transformer->h->43->ln_1', 'transformer->h->43->attn', 'transformer->h->43->attn->c_attn', 'transformer->h->43->attn->c_proj', 'transformer->h->43->attn->attn_dropout', 'transformer->h->43->attn->resid_dropout', 'transformer->h->43->ln_2', 'transformer->h->43->mlp', 'transformer->h->43->mlp->c_fc', 'transformer->h->43->mlp->c_proj', 'transformer->h->43->mlp->act', 'transformer->h->43->mlp->dropout', 'transformer->h->44', 'transformer->h->44->ln_1', 'transformer->h->44->attn', 'transformer->h->44->attn->c_attn', 'transformer->h->44->attn->c_proj', 'transformer->h->44->attn->attn_dropout', 'transformer->h->44->attn->resid_dropout', 'transformer->h->44->ln_2', 'transformer->h->44->mlp', 'transformer->h->44->mlp->c_fc', 'transformer->h->44->mlp->c_proj', 'transformer->h->44->mlp->act', 'transformer->h->44->mlp->dropout', 'transformer->h->45', 'transformer->h->45->ln_1', 'transformer->h->45->attn', 'transformer->h->45->attn->c_attn', 'transformer->h->45->attn->c_proj', 'transformer->h->45->attn->attn_dropout', 'transformer->h->45->attn->resid_dropout', 'transformer->h->45->ln_2', 'transformer->h->45->mlp', 'transformer->h->45->mlp->c_fc', 'transformer->h->45->mlp->c_proj', 'transformer->h->45->mlp->act', 'transformer->h->45->mlp->dropout', 'transformer->h->46', 'transformer->h->46->ln_1', 'transformer->h->46->attn', 'transformer->h->46->attn->c_attn', 'transformer->h->46->attn->c_proj', 'transformer->h->46->attn->attn_dropout', 'transformer->h->46->attn->resid_dropout', 'transformer->h->46->ln_2', 'transformer->h->46->mlp', 'transformer->h->46->mlp->c_fc', 'transformer->h->46->mlp->c_proj', 'transformer->h->46->mlp->act', 'transformer->h->46->mlp->dropout', 'transformer->h->47', 'transformer->h->47->ln_1', 'transformer->h->47->attn', 'transformer->h->47->attn->c_attn', 'transformer->h->47->attn->c_proj', 'transformer->h->47->attn->attn_dropout', 'transformer->h->47->attn->resid_dropout', 'transformer->h->47->ln_2', 'transformer->h->47->mlp', 'transformer->h->47->mlp->c_fc', 'transformer->h->47->mlp->c_proj', 'transformer->h->47->mlp->act', 'transformer->h->47->mlp->dropout', 'transformer->ln_f', 'lm_head']\n"
     ]
    }
   ],
   "source": [
    "# layer names\n",
    "print(list(model.layers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "f = open(\"../../../datasets/known_1000.json\")\n",
    "prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vinson Massif', 'Vinson Massif is located in the continent of Antarctica')\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "texts = [(prompts[i]['subject'], prompts[i]['prompt'] + ' ' + prompts[i]['attribute']) for i in range(len(prompts))]\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rome_examples = [\"Brian De Palma works in the area of film\", \\\n",
    "                 \"Germaine Greer's domain of work is feminism\", \\\n",
    "                 \"Lawrence Taylor professionally plays the sport of football\", \\\n",
    "                 \"The headquarter of Zillow is in downtown Seattle\", \\\n",
    "                 \"Lexus's owner, Toyota\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens:  9 text:  Brian De Palma works in the area of film\n",
      "num tokens:  5 text:  Lexus's owner, Toyota\n",
      "num tokens:  10 text:  The headquarter of Zillow is in downtown Seattle\n",
      "num tokens:  10 text:  Germaine Greer's domain of work is feminism\n",
      "num tokens:  8 text:  Lawrence Taylor professionally plays the sport of football\n"
     ]
    }
   ],
   "source": [
    "### Batching requires inputs to be of the same size so we put each prompt in \n",
    "### a bucket depending on the number of tokens it contains.\n",
    "\n",
    "inputs = [[] for z in range(30)] ## use 30 as an upper bound on the number of tokens\n",
    "for subject, text in texts:\n",
    "    if text not in rome_examples:\n",
    "        continue\n",
    "    encoded_text, target_id = utility.prepare_input(text, tokenizer, device)\n",
    "    num_tokens = encoded_text['input_ids'].shape[1]\n",
    "    \n",
    "    print(\"num tokens: \", num_tokens, \"text: \", text)\n",
    "    \n",
    "    # get tokens\n",
    "    input_ids = encoded_text['input_ids'][0]\n",
    "    tkens = [tokenizer.decode(input_ids[i]) for i in range(len(input_ids))]\n",
    "    \n",
    "    # get position of subject tokens\n",
    "    subject_positions = utility.get_subject_positions(tkens, subject)\n",
    "    indx_str = str(subject_positions[0]) + \":\" + str(subject_positions[-1]+1)\n",
    "    \n",
    "    # add star to subject tokens to later do analysis\n",
    "    start, end = subject_positions[0], subject_positions[1]\n",
    "    while start < end:\n",
    "        tkens[start] = tkens[start] + \"*\"\n",
    "        start += 1\n",
    "    \n",
    "    # save uncorrupted states for patching\n",
    "    num_layers = get_num_layers(model, layer_key_prefix='transformer->h')\n",
    "    output_hooks_names = [f'transformer->h->{layer}' for layer in range(num_layers)]\n",
    "    output_hooks = [Hook(name, utility.save_output, name) for name in output_hooks_names]\n",
    "    model(**encoded_text, hooks=output_hooks)\n",
    "    \n",
    "    hidden_states = [model.save_ctx[f'transformer->h->{layer}']['output'][0].detach() for layer in range(num_layers)] # need detach\n",
    "    \n",
    "    # get probability with uncorrupted subject\n",
    "    p = model(**encoded_text, hooks=[])['logits'].softmax(dim=-1)[0,-1,target_id].item()\n",
    "    \n",
    "    NUM_RUNS = 10 # ROME runs each input 10 times\n",
    "    for i in range(NUM_RUNS):\n",
    "        # add noise hook\n",
    "        seed = np.random.randint(100, size=1)[0]\n",
    "        noise_hook = Hook(\n",
    "            layer_name='transformer->wte',\n",
    "            func=hooks.additive_output_noise(indices=indx_str, mean=0, std=0.1, seed=seed),\n",
    "            key='embedding_noise',\n",
    "        )\n",
    "    \n",
    "        # get probability with corrupted subject\n",
    "        p_star = model(**encoded_text, hooks=[noise_hook])['logits'].softmax(dim=-1)[0,-1,target_id].item()\n",
    "        \n",
    "        inputs[num_tokens].append((num_tokens, num_layers, encoded_text, hidden_states, tkens, text, target_id, indx_str, seed, p, p_star))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens:  5 number of prompts:  10\n",
      "num tokens:  8 number of prompts:  10\n",
      "num tokens:  9 number of prompts:  10\n",
      "num tokens:  10 number of prompts:  20\n"
     ]
    }
   ],
   "source": [
    "# statistics on number of prompts for each length tokens\n",
    "for i in range(len(inputs)):\n",
    "    length = len(inputs[i])\n",
    "    if length != 0:\n",
    "        print(\"num tokens: \", i, \"number of prompts: \", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove input buckets with 0 prompts\n",
    "inputs = [inp for inp in inputs if inp != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in this bucket: 10.\n",
      "This batch has 10 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec120850a4aa460f84577892a5583a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in this bucket: 10.\n",
      "This batch has 10 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149e415db977442c9848ef602e863263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in this bucket: 10.\n",
      "This batch has 10 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40a71c99e4a40fb8376b976fa2b5bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in this bucket: 20.\n",
      "This batch has 20 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08c6c09a03e4ddcb23c0d0094c94d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "batch_schedule = {i : int(1000/i) for i in range(1, 30)}\n",
    "for inp in inputs:\n",
    "    print(f\"Number of prompts in this bucket: {len(inp)}.\")\n",
    "    \n",
    "    batch_size = batch_schedule[inp[0][0]]\n",
    "    batched_data = DataLoader(inp, batch_size=batch_size)\n",
    "    for data in batched_data:\n",
    "        curr_batch_size = data[0].size()[0]\n",
    "        print(\"This batch has\", curr_batch_size, \"examples.\")\n",
    "        \n",
    "        num_tokens = data[0][0].item()\n",
    "        num_layers = data[1][0].item()\n",
    "        encoded_text = data[2]\n",
    "        hidden_states = data[3]\n",
    "        tkens = [[t[i] for t in data[4]] for i in range(curr_batch_size)]\n",
    "        text = [t for t in data[5]]\n",
    "        target_ids = [t.item() for t in data[6]]\n",
    "        indx_strs = [s for s in data[7]]\n",
    "        seeds = [seed.item() for seed in data[8]]\n",
    "        p = [p.item() for p in data[9]]\n",
    "        p_stars = [p_s.item() for p_s in data[10]]\n",
    "\n",
    "        noise_hooks = [] # can probably be batched [will take a look later on]\n",
    "        for i in range(curr_batch_size):\n",
    "            noise_hook = Hook(\n",
    "                layer_name='transformer->wte',\n",
    "                func=hooks.additive_output_noise(indices=indx_strs[i], mean=0, std=0.1, index=i, seed=seeds[i]),\n",
    "                key='embedding_noise',\n",
    "            )\n",
    "            noise_hooks.append(noise_hook)\n",
    "            \n",
    "        for layer in tqdm(range(num_layers)):\n",
    "            for position in range(num_tokens):\n",
    "                hook = Hook(\n",
    "                    layer_name=f'transformer->h->{layer}',\n",
    "                    func= hooks.hidden_patch_hook_fn(layer, position, hidden_states),\n",
    "                    key=f'patch_{layer}_pos{position}'\n",
    "                )\n",
    "                output = model(**encoded_text, hooks=noise_hooks+[hook])\n",
    "                for i in range(curr_batch_size):\n",
    "                    if text[i] not in results:\n",
    "                        results[text[i]] = (torch.zeros((num_tokens, num_layers)), (text[i], tkens[i], num_layers, sum(p) / len(p), sum(p_stars) / len(p_stars)))\n",
    "                    results[text[i]][0][position, layer] += torch.softmax(output[\"logits\"][i][0,-1,:], 0)[target_ids[i]].item() - p_stars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to divide by NUM_RUNS\n",
    "res = []\n",
    "for _, value in results.items():\n",
    "    result, accessory = value\n",
    "    result = result / NUM_RUNS\n",
    "    res.append((result, accessory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(res, \"../../data/gpt2-xl/rome_examples.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 25 16:32:12 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:52:00.0 Off |                    0 |\n",
      "| N/A   52C    P0    87W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  On   | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    53W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unseal -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from statistics import stdev\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import unseal\n",
    "from unseal.hooks import Hook, HookedModel, common_hooks\n",
    "from unseal.transformers_util import load_from_pretrained, get_num_layers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utility\n",
    "import hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhooked_model, tokenizer, config = load_from_pretrained('EleutherAI/gpt-j-6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and on device cuda!\n"
     ]
    }
   ],
   "source": [
    "model = HookedModel(unhooked_model)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(f'Model loaded and on device {device}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'transformer->wte', 'transformer->drop', 'transformer->h', 'transformer->h->0', 'transformer->h->0->ln_1', 'transformer->h->0->attn', 'transformer->h->0->attn->attn_dropout', 'transformer->h->0->attn->resid_dropout', 'transformer->h->0->attn->k_proj', 'transformer->h->0->attn->v_proj', 'transformer->h->0->attn->q_proj', 'transformer->h->0->attn->out_proj', 'transformer->h->0->mlp', 'transformer->h->0->mlp->fc_in', 'transformer->h->0->mlp->fc_out', 'transformer->h->0->mlp->act', 'transformer->h->0->mlp->dropout', 'transformer->h->1', 'transformer->h->1->ln_1', 'transformer->h->1->attn', 'transformer->h->1->attn->attn_dropout', 'transformer->h->1->attn->resid_dropout', 'transformer->h->1->attn->k_proj', 'transformer->h->1->attn->v_proj', 'transformer->h->1->attn->q_proj', 'transformer->h->1->attn->out_proj', 'transformer->h->1->mlp', 'transformer->h->1->mlp->fc_in', 'transformer->h->1->mlp->fc_out', 'transformer->h->1->mlp->act', 'transformer->h->1->mlp->dropout', 'transformer->h->2', 'transformer->h->2->ln_1', 'transformer->h->2->attn', 'transformer->h->2->attn->attn_dropout', 'transformer->h->2->attn->resid_dropout', 'transformer->h->2->attn->k_proj', 'transformer->h->2->attn->v_proj', 'transformer->h->2->attn->q_proj', 'transformer->h->2->attn->out_proj', 'transformer->h->2->mlp', 'transformer->h->2->mlp->fc_in', 'transformer->h->2->mlp->fc_out', 'transformer->h->2->mlp->act', 'transformer->h->2->mlp->dropout', 'transformer->h->3', 'transformer->h->3->ln_1', 'transformer->h->3->attn', 'transformer->h->3->attn->attn_dropout', 'transformer->h->3->attn->resid_dropout', 'transformer->h->3->attn->k_proj', 'transformer->h->3->attn->v_proj', 'transformer->h->3->attn->q_proj', 'transformer->h->3->attn->out_proj', 'transformer->h->3->mlp', 'transformer->h->3->mlp->fc_in', 'transformer->h->3->mlp->fc_out', 'transformer->h->3->mlp->act', 'transformer->h->3->mlp->dropout', 'transformer->h->4', 'transformer->h->4->ln_1', 'transformer->h->4->attn', 'transformer->h->4->attn->attn_dropout', 'transformer->h->4->attn->resid_dropout', 'transformer->h->4->attn->k_proj', 'transformer->h->4->attn->v_proj', 'transformer->h->4->attn->q_proj', 'transformer->h->4->attn->out_proj', 'transformer->h->4->mlp', 'transformer->h->4->mlp->fc_in', 'transformer->h->4->mlp->fc_out', 'transformer->h->4->mlp->act', 'transformer->h->4->mlp->dropout', 'transformer->h->5', 'transformer->h->5->ln_1', 'transformer->h->5->attn', 'transformer->h->5->attn->attn_dropout', 'transformer->h->5->attn->resid_dropout', 'transformer->h->5->attn->k_proj', 'transformer->h->5->attn->v_proj', 'transformer->h->5->attn->q_proj', 'transformer->h->5->attn->out_proj', 'transformer->h->5->mlp', 'transformer->h->5->mlp->fc_in', 'transformer->h->5->mlp->fc_out', 'transformer->h->5->mlp->act', 'transformer->h->5->mlp->dropout', 'transformer->h->6', 'transformer->h->6->ln_1', 'transformer->h->6->attn', 'transformer->h->6->attn->attn_dropout', 'transformer->h->6->attn->resid_dropout', 'transformer->h->6->attn->k_proj', 'transformer->h->6->attn->v_proj', 'transformer->h->6->attn->q_proj', 'transformer->h->6->attn->out_proj', 'transformer->h->6->mlp', 'transformer->h->6->mlp->fc_in', 'transformer->h->6->mlp->fc_out', 'transformer->h->6->mlp->act', 'transformer->h->6->mlp->dropout', 'transformer->h->7', 'transformer->h->7->ln_1', 'transformer->h->7->attn', 'transformer->h->7->attn->attn_dropout', 'transformer->h->7->attn->resid_dropout', 'transformer->h->7->attn->k_proj', 'transformer->h->7->attn->v_proj', 'transformer->h->7->attn->q_proj', 'transformer->h->7->attn->out_proj', 'transformer->h->7->mlp', 'transformer->h->7->mlp->fc_in', 'transformer->h->7->mlp->fc_out', 'transformer->h->7->mlp->act', 'transformer->h->7->mlp->dropout', 'transformer->h->8', 'transformer->h->8->ln_1', 'transformer->h->8->attn', 'transformer->h->8->attn->attn_dropout', 'transformer->h->8->attn->resid_dropout', 'transformer->h->8->attn->k_proj', 'transformer->h->8->attn->v_proj', 'transformer->h->8->attn->q_proj', 'transformer->h->8->attn->out_proj', 'transformer->h->8->mlp', 'transformer->h->8->mlp->fc_in', 'transformer->h->8->mlp->fc_out', 'transformer->h->8->mlp->act', 'transformer->h->8->mlp->dropout', 'transformer->h->9', 'transformer->h->9->ln_1', 'transformer->h->9->attn', 'transformer->h->9->attn->attn_dropout', 'transformer->h->9->attn->resid_dropout', 'transformer->h->9->attn->k_proj', 'transformer->h->9->attn->v_proj', 'transformer->h->9->attn->q_proj', 'transformer->h->9->attn->out_proj', 'transformer->h->9->mlp', 'transformer->h->9->mlp->fc_in', 'transformer->h->9->mlp->fc_out', 'transformer->h->9->mlp->act', 'transformer->h->9->mlp->dropout', 'transformer->h->10', 'transformer->h->10->ln_1', 'transformer->h->10->attn', 'transformer->h->10->attn->attn_dropout', 'transformer->h->10->attn->resid_dropout', 'transformer->h->10->attn->k_proj', 'transformer->h->10->attn->v_proj', 'transformer->h->10->attn->q_proj', 'transformer->h->10->attn->out_proj', 'transformer->h->10->mlp', 'transformer->h->10->mlp->fc_in', 'transformer->h->10->mlp->fc_out', 'transformer->h->10->mlp->act', 'transformer->h->10->mlp->dropout', 'transformer->h->11', 'transformer->h->11->ln_1', 'transformer->h->11->attn', 'transformer->h->11->attn->attn_dropout', 'transformer->h->11->attn->resid_dropout', 'transformer->h->11->attn->k_proj', 'transformer->h->11->attn->v_proj', 'transformer->h->11->attn->q_proj', 'transformer->h->11->attn->out_proj', 'transformer->h->11->mlp', 'transformer->h->11->mlp->fc_in', 'transformer->h->11->mlp->fc_out', 'transformer->h->11->mlp->act', 'transformer->h->11->mlp->dropout', 'transformer->h->12', 'transformer->h->12->ln_1', 'transformer->h->12->attn', 'transformer->h->12->attn->attn_dropout', 'transformer->h->12->attn->resid_dropout', 'transformer->h->12->attn->k_proj', 'transformer->h->12->attn->v_proj', 'transformer->h->12->attn->q_proj', 'transformer->h->12->attn->out_proj', 'transformer->h->12->mlp', 'transformer->h->12->mlp->fc_in', 'transformer->h->12->mlp->fc_out', 'transformer->h->12->mlp->act', 'transformer->h->12->mlp->dropout', 'transformer->h->13', 'transformer->h->13->ln_1', 'transformer->h->13->attn', 'transformer->h->13->attn->attn_dropout', 'transformer->h->13->attn->resid_dropout', 'transformer->h->13->attn->k_proj', 'transformer->h->13->attn->v_proj', 'transformer->h->13->attn->q_proj', 'transformer->h->13->attn->out_proj', 'transformer->h->13->mlp', 'transformer->h->13->mlp->fc_in', 'transformer->h->13->mlp->fc_out', 'transformer->h->13->mlp->act', 'transformer->h->13->mlp->dropout', 'transformer->h->14', 'transformer->h->14->ln_1', 'transformer->h->14->attn', 'transformer->h->14->attn->attn_dropout', 'transformer->h->14->attn->resid_dropout', 'transformer->h->14->attn->k_proj', 'transformer->h->14->attn->v_proj', 'transformer->h->14->attn->q_proj', 'transformer->h->14->attn->out_proj', 'transformer->h->14->mlp', 'transformer->h->14->mlp->fc_in', 'transformer->h->14->mlp->fc_out', 'transformer->h->14->mlp->act', 'transformer->h->14->mlp->dropout', 'transformer->h->15', 'transformer->h->15->ln_1', 'transformer->h->15->attn', 'transformer->h->15->attn->attn_dropout', 'transformer->h->15->attn->resid_dropout', 'transformer->h->15->attn->k_proj', 'transformer->h->15->attn->v_proj', 'transformer->h->15->attn->q_proj', 'transformer->h->15->attn->out_proj', 'transformer->h->15->mlp', 'transformer->h->15->mlp->fc_in', 'transformer->h->15->mlp->fc_out', 'transformer->h->15->mlp->act', 'transformer->h->15->mlp->dropout', 'transformer->h->16', 'transformer->h->16->ln_1', 'transformer->h->16->attn', 'transformer->h->16->attn->attn_dropout', 'transformer->h->16->attn->resid_dropout', 'transformer->h->16->attn->k_proj', 'transformer->h->16->attn->v_proj', 'transformer->h->16->attn->q_proj', 'transformer->h->16->attn->out_proj', 'transformer->h->16->mlp', 'transformer->h->16->mlp->fc_in', 'transformer->h->16->mlp->fc_out', 'transformer->h->16->mlp->act', 'transformer->h->16->mlp->dropout', 'transformer->h->17', 'transformer->h->17->ln_1', 'transformer->h->17->attn', 'transformer->h->17->attn->attn_dropout', 'transformer->h->17->attn->resid_dropout', 'transformer->h->17->attn->k_proj', 'transformer->h->17->attn->v_proj', 'transformer->h->17->attn->q_proj', 'transformer->h->17->attn->out_proj', 'transformer->h->17->mlp', 'transformer->h->17->mlp->fc_in', 'transformer->h->17->mlp->fc_out', 'transformer->h->17->mlp->act', 'transformer->h->17->mlp->dropout', 'transformer->h->18', 'transformer->h->18->ln_1', 'transformer->h->18->attn', 'transformer->h->18->attn->attn_dropout', 'transformer->h->18->attn->resid_dropout', 'transformer->h->18->attn->k_proj', 'transformer->h->18->attn->v_proj', 'transformer->h->18->attn->q_proj', 'transformer->h->18->attn->out_proj', 'transformer->h->18->mlp', 'transformer->h->18->mlp->fc_in', 'transformer->h->18->mlp->fc_out', 'transformer->h->18->mlp->act', 'transformer->h->18->mlp->dropout', 'transformer->h->19', 'transformer->h->19->ln_1', 'transformer->h->19->attn', 'transformer->h->19->attn->attn_dropout', 'transformer->h->19->attn->resid_dropout', 'transformer->h->19->attn->k_proj', 'transformer->h->19->attn->v_proj', 'transformer->h->19->attn->q_proj', 'transformer->h->19->attn->out_proj', 'transformer->h->19->mlp', 'transformer->h->19->mlp->fc_in', 'transformer->h->19->mlp->fc_out', 'transformer->h->19->mlp->act', 'transformer->h->19->mlp->dropout', 'transformer->h->20', 'transformer->h->20->ln_1', 'transformer->h->20->attn', 'transformer->h->20->attn->attn_dropout', 'transformer->h->20->attn->resid_dropout', 'transformer->h->20->attn->k_proj', 'transformer->h->20->attn->v_proj', 'transformer->h->20->attn->q_proj', 'transformer->h->20->attn->out_proj', 'transformer->h->20->mlp', 'transformer->h->20->mlp->fc_in', 'transformer->h->20->mlp->fc_out', 'transformer->h->20->mlp->act', 'transformer->h->20->mlp->dropout', 'transformer->h->21', 'transformer->h->21->ln_1', 'transformer->h->21->attn', 'transformer->h->21->attn->attn_dropout', 'transformer->h->21->attn->resid_dropout', 'transformer->h->21->attn->k_proj', 'transformer->h->21->attn->v_proj', 'transformer->h->21->attn->q_proj', 'transformer->h->21->attn->out_proj', 'transformer->h->21->mlp', 'transformer->h->21->mlp->fc_in', 'transformer->h->21->mlp->fc_out', 'transformer->h->21->mlp->act', 'transformer->h->21->mlp->dropout', 'transformer->h->22', 'transformer->h->22->ln_1', 'transformer->h->22->attn', 'transformer->h->22->attn->attn_dropout', 'transformer->h->22->attn->resid_dropout', 'transformer->h->22->attn->k_proj', 'transformer->h->22->attn->v_proj', 'transformer->h->22->attn->q_proj', 'transformer->h->22->attn->out_proj', 'transformer->h->22->mlp', 'transformer->h->22->mlp->fc_in', 'transformer->h->22->mlp->fc_out', 'transformer->h->22->mlp->act', 'transformer->h->22->mlp->dropout', 'transformer->h->23', 'transformer->h->23->ln_1', 'transformer->h->23->attn', 'transformer->h->23->attn->attn_dropout', 'transformer->h->23->attn->resid_dropout', 'transformer->h->23->attn->k_proj', 'transformer->h->23->attn->v_proj', 'transformer->h->23->attn->q_proj', 'transformer->h->23->attn->out_proj', 'transformer->h->23->mlp', 'transformer->h->23->mlp->fc_in', 'transformer->h->23->mlp->fc_out', 'transformer->h->23->mlp->act', 'transformer->h->23->mlp->dropout', 'transformer->h->24', 'transformer->h->24->ln_1', 'transformer->h->24->attn', 'transformer->h->24->attn->attn_dropout', 'transformer->h->24->attn->resid_dropout', 'transformer->h->24->attn->k_proj', 'transformer->h->24->attn->v_proj', 'transformer->h->24->attn->q_proj', 'transformer->h->24->attn->out_proj', 'transformer->h->24->mlp', 'transformer->h->24->mlp->fc_in', 'transformer->h->24->mlp->fc_out', 'transformer->h->24->mlp->act', 'transformer->h->24->mlp->dropout', 'transformer->h->25', 'transformer->h->25->ln_1', 'transformer->h->25->attn', 'transformer->h->25->attn->attn_dropout', 'transformer->h->25->attn->resid_dropout', 'transformer->h->25->attn->k_proj', 'transformer->h->25->attn->v_proj', 'transformer->h->25->attn->q_proj', 'transformer->h->25->attn->out_proj', 'transformer->h->25->mlp', 'transformer->h->25->mlp->fc_in', 'transformer->h->25->mlp->fc_out', 'transformer->h->25->mlp->act', 'transformer->h->25->mlp->dropout', 'transformer->h->26', 'transformer->h->26->ln_1', 'transformer->h->26->attn', 'transformer->h->26->attn->attn_dropout', 'transformer->h->26->attn->resid_dropout', 'transformer->h->26->attn->k_proj', 'transformer->h->26->attn->v_proj', 'transformer->h->26->attn->q_proj', 'transformer->h->26->attn->out_proj', 'transformer->h->26->mlp', 'transformer->h->26->mlp->fc_in', 'transformer->h->26->mlp->fc_out', 'transformer->h->26->mlp->act', 'transformer->h->26->mlp->dropout', 'transformer->h->27', 'transformer->h->27->ln_1', 'transformer->h->27->attn', 'transformer->h->27->attn->attn_dropout', 'transformer->h->27->attn->resid_dropout', 'transformer->h->27->attn->k_proj', 'transformer->h->27->attn->v_proj', 'transformer->h->27->attn->q_proj', 'transformer->h->27->attn->out_proj', 'transformer->h->27->mlp', 'transformer->h->27->mlp->fc_in', 'transformer->h->27->mlp->fc_out', 'transformer->h->27->mlp->act', 'transformer->h->27->mlp->dropout', 'transformer->ln_f', 'lm_head']\n"
     ]
    }
   ],
   "source": [
    "# layer names\n",
    "print(list(model.layers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "f = open(\"../../data/known_1000.json\")\n",
    "prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vinson Massif', 'Vinson Massif is located in the continent of Antarctica')\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "texts = [(prompts[i]['subject'], prompts[i]['prompt'] + ' ' + prompts[i]['attribute']) for i in range(len(prompts))]\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens:  6 text:  iPod Touch is developed by Apple\n"
     ]
    }
   ],
   "source": [
    "### Batching requires inputs to be of the same size so we put each prompt in \n",
    "### a bucket depending on the number of tokens it contains.\n",
    "\n",
    "inputs = [[] for z in range(30)] ## use 30 as an upper bound on the number of tokens\n",
    "for subject, text in random.sample(texts, 1):\n",
    "    encoded_text, target_id = utility.prepare_input(text, tokenizer, device)\n",
    "    num_tokens = encoded_text['input_ids'].shape[1]\n",
    "    \n",
    "    print(\"num tokens: \", num_tokens, \"text: \", text)\n",
    "    \n",
    "    # get tokens\n",
    "    input_ids = encoded_text['input_ids'][0]\n",
    "    tkens = [tokenizer.decode(input_ids[i]) for i in range(len(input_ids))]\n",
    "    \n",
    "    # get position of subject tokens\n",
    "    subject_positions = utility.get_subject_positions(tkens, subject)\n",
    "    indx_str = str(subject_positions[0]) + \":\" + str(subject_positions[-1]+1)\n",
    "    \n",
    "    # add star to subject tokens to later do analysis\n",
    "    start, end = subject_positions[0], subject_positions[1]\n",
    "    while start < end:\n",
    "        tkens[start] = tkens[start] + \"*\"\n",
    "        start += 1\n",
    "    \n",
    "    # save uncorrupted states for patching\n",
    "    num_layers = get_num_layers(model, layer_key_prefix='transformer->h')\n",
    "    output_hooks_names = [f'transformer->h->{layer}' for layer in range(num_layers)]\n",
    "    output_hooks = [Hook(name, utility.save_output, name) for name in output_hooks_names]\n",
    "    model(**encoded_text, hooks=output_hooks)\n",
    "    \n",
    "    hidden_states = [model.save_ctx[f'transformer->h->{layer}']['output'][0].detach() for layer in range(num_layers)] # need detach\n",
    "    \n",
    "    # get probability with uncorrupted subject\n",
    "    p = model(**encoded_text, hooks=[])['logits'].softmax(dim=-1)[0,-1,target_id].item()\n",
    "    \n",
    "    NUM_RUNS = 10 # ROME runs each input 10 times\n",
    "    for i in range(NUM_RUNS):\n",
    "        # add noise hook\n",
    "        seed = np.random.randint(100, size=1)[0]\n",
    "        noise_hook = Hook(\n",
    "            layer_name='transformer->wte',\n",
    "            func=hooks.additive_output_noise(indices=indx_str, mean=0, std=0.1, seed=seed),\n",
    "            key='embedding_noise',\n",
    "        )\n",
    "    \n",
    "        # get probability with corrupted subject\n",
    "        p_star = model(**encoded_text, hooks=[noise_hook])['logits'].softmax(dim=-1)[0,-1,target_id].item()\n",
    "        \n",
    "        inputs[num_tokens].append((num_tokens, num_layers, encoded_text, hidden_states, tkens, text, target_id, indx_str, seed, p, p_star))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens:  6 number of prompts:  1\n"
     ]
    }
   ],
   "source": [
    "# statistics on number of prompts for each length tokens\n",
    "for i in range(len(inputs)):\n",
    "    length = len(inputs[i])\n",
    "    if length != 0:\n",
    "        print(\"num tokens: \", i, \"number of prompts: \", int(length/NUM_RUNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove input buckets with 0 prompts\n",
    "inputs = [inp for inp in inputs if inp != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with longest prompts first :)\n",
    "inputs.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in this bucket: 10.\n",
      "This batch has 10 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e181a2a37824ff18eac938402616c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "results = {}\n",
    "batch_schedule = {i : int(1250/i) for i in range(1, 30)}\n",
    "for inp in inputs:\n",
    "    print(f\"Number of prompts in this bucket: {len(inp)}.\")\n",
    "    \n",
    "    batch_size = batch_schedule[inp[0][0]]\n",
    "    batched_data = DataLoader(inp, batch_size=batch_size)\n",
    "    for data in batched_data:\n",
    "        curr_batch_size = data[0].size()[0]\n",
    "        print(\"This batch has\", curr_batch_size, \"examples.\")\n",
    "        \n",
    "        num_tokens = data[0][0].item()\n",
    "        num_layers = data[1][0].item()\n",
    "        encoded_text = data[2]\n",
    "        hidden_states = data[3]\n",
    "        tkens = [[t[i] for t in data[4]] for i in range(curr_batch_size)]\n",
    "        text = [t for t in data[5]]\n",
    "        target_ids = [t.item() for t in data[6]]\n",
    "        indx_strs = [s for s in data[7]]\n",
    "        seeds = [seed.item() for seed in data[8]]\n",
    "        p = [p.item() for p in data[9]]\n",
    "        p_stars = [p_s.item() for p_s in data[10]]\n",
    "\n",
    "        noise_hooks = [] # can probably be batched [will take a look later on]\n",
    "        for i in range(curr_batch_size):\n",
    "            noise_hook = Hook(\n",
    "                layer_name='transformer->wte',\n",
    "                func=hooks.additive_output_noise(indices=indx_strs[i], mean=0, std=0.1, index=i, seed=seeds[i]),\n",
    "                key='embedding_noise',\n",
    "            )\n",
    "            noise_hooks.append(noise_hook)\n",
    "            \n",
    "        for layer in tqdm(range(num_layers)):\n",
    "            for position in range(num_tokens):\n",
    "                hook = Hook(\n",
    "                    layer_name=f'transformer->h->{layer}',\n",
    "                    func= hooks.hidden_patch_hook_fn(layer, position, hidden_states),\n",
    "                    key=f'patch_{layer}_pos{position}'\n",
    "                )\n",
    "                output = model(**encoded_text, hooks=noise_hooks+[hook])\n",
    "                for i in range(curr_batch_size):\n",
    "                    if text[i] not in results:\n",
    "                        results[text[i]] = (torch.zeros((num_tokens, num_layers)), (tkens[i], num_layers, sum(p) / len(p), sum(p_stars) / len(p_stars)))\n",
    "                    results[text[i]][0][position, layer] += torch.softmax(output[\"logits\"][i][0,-1,:], 0)[target_ids[i]].item() - p_stars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to divide by NUM_RUNS\n",
    "res = []\n",
    "for text, value in results.items():\n",
    "    result, access = value\n",
    "    result = result / NUM_RUNS\n",
    "    res.append((result, access))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(res, \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(\"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[ 1.4021e-03,  1.8721e-03,  1.0469e-03,  3.5102e-04,  1.0320e-04,\n",
       "            7.8951e-05,  1.2838e-06, -1.9918e-05,  4.8063e-05,  9.1360e-05,\n",
       "            7.8987e-05,  7.9560e-05,  4.5273e-05,  1.9115e-05,  1.0897e-04,\n",
       "            4.7719e-05,  3.4097e-05,  8.0088e-05,  4.1766e-05,  2.6896e-05,\n",
       "           -8.9898e-07, -1.1589e-05,  1.1699e-05,  1.8563e-05,  6.5224e-06,\n",
       "            2.9394e-05,  1.2499e-06,  4.4009e-08],\n",
       "          [ 6.6839e-05,  1.8172e-02,  1.8861e-02,  3.3730e-02,  2.0518e-02,\n",
       "            2.0796e-02,  4.2322e-02,  4.3422e-02,  3.6366e-02,  1.8828e-02,\n",
       "            2.1778e-02,  1.9032e-02,  1.6895e-02,  1.7516e-02,  1.3520e-02,\n",
       "            1.4064e-02,  5.9119e-03,  5.0641e-03,  5.1391e-03,  4.9349e-03,\n",
       "            2.2478e-03,  1.9256e-03,  1.4715e-03,  1.8158e-05, -4.7110e-06,\n",
       "            1.5572e-05,  4.9942e-06,  4.4009e-08],\n",
       "          [ 5.4772e-03,  4.2587e-02,  5.2087e-02,  1.3120e-01,  1.3283e-01,\n",
       "            1.9141e-01,  1.6652e-01,  1.8327e-01,  1.6097e-01,  5.4122e-02,\n",
       "            4.0511e-02,  1.4076e-02,  1.2351e-02,  1.2466e-02,  5.4772e-03,\n",
       "            6.0361e-03,  2.0747e-03,  1.4819e-03,  1.4495e-03,  1.3110e-03,\n",
       "            8.3760e-04,  2.4144e-04,  2.0403e-04,  1.8968e-05,  1.2336e-05,\n",
       "            2.3450e-05,  2.0594e-05,  4.4009e-08],\n",
       "          [ 1.3028e-04,  1.4082e-04,  5.5290e-05,  5.6272e-05,  2.2792e-06,\n",
       "            3.6902e-05,  4.0296e-05,  2.3224e-04,  1.7994e-04,  5.0621e-04,\n",
       "            7.9027e-04,  1.6863e-03,  2.7267e-03,  4.5004e-03,  1.2781e-03,\n",
       "            1.0747e-03,  1.0722e-03,  4.6996e-04,  3.0765e-04,  3.0888e-04,\n",
       "            2.9279e-04,  9.7603e-05,  1.0609e-04,  9.9952e-05,  5.5067e-05,\n",
       "            6.2408e-05,  7.8876e-06,  4.4009e-08],\n",
       "          [-4.5269e-04, -4.4865e-04, -4.4139e-04, -1.7318e-04,  8.4517e-05,\n",
       "            2.4052e-04,  5.4833e-04,  6.9891e-04,  3.4119e-04,  6.7813e-04,\n",
       "            1.1212e-03,  1.9660e-03,  4.2666e-03,  8.0789e-03,  1.5962e-03,\n",
       "            1.6622e-03,  1.7203e-03,  6.1527e-04,  3.7172e-04,  4.5075e-04,\n",
       "            3.5645e-04,  1.8150e-04,  1.8146e-04,  1.3688e-04,  8.4795e-05,\n",
       "            5.5417e-05,  2.9321e-05,  4.4009e-08],\n",
       "          [-6.6067e-05, -4.7405e-05, -1.7255e-04, -4.9250e-05, -4.0404e-05,\n",
       "           -5.9666e-05,  7.4260e-05,  1.2069e-04,  4.1255e-04,  3.0493e-03,\n",
       "            4.4869e-03,  1.1531e-02,  1.9030e-01,  3.5359e-01,  3.8378e-01,\n",
       "            4.5054e-01,  6.9463e-01,  7.0277e-01,  7.1870e-01,  7.2211e-01,\n",
       "            7.4273e-01,  7.9052e-01,  8.0272e-01,  8.0874e-01,  8.1909e-01,\n",
       "            8.2168e-01,  8.4101e-01,  8.4102e-01]]),\n",
       "  (['i*', 'Pod*', ' Touch*', ' is', ' developed', ' by'],\n",
       "   28,\n",
       "   0.8414985537528992,\n",
       "   0.0004776019724886282))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

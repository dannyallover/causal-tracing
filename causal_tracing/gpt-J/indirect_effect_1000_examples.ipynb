{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 25 16:43:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:52:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    79W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  On   | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    53W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unseal -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from statistics import stdev\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import unseal\n",
    "from unseal.hooks import Hook, HookedModel, common_hooks\n",
    "from unseal.transformers_util import load_from_pretrained, get_num_layers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utility\n",
    "import hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhooked_model, tokenizer, config = load_from_pretrained('EleutherAI/gpt-j-6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and on device cuda!\n"
     ]
    }
   ],
   "source": [
    "model = HookedModel(unhooked_model)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(f'Model loaded and on device {device}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'transformer->wte', 'transformer->drop', 'transformer->h', 'transformer->h->0', 'transformer->h->0->ln_1', 'transformer->h->0->attn', 'transformer->h->0->attn->attn_dropout', 'transformer->h->0->attn->resid_dropout', 'transformer->h->0->attn->k_proj', 'transformer->h->0->attn->v_proj', 'transformer->h->0->attn->q_proj', 'transformer->h->0->attn->out_proj', 'transformer->h->0->mlp', 'transformer->h->0->mlp->fc_in', 'transformer->h->0->mlp->fc_out', 'transformer->h->0->mlp->act', 'transformer->h->0->mlp->dropout', 'transformer->h->1', 'transformer->h->1->ln_1', 'transformer->h->1->attn', 'transformer->h->1->attn->attn_dropout', 'transformer->h->1->attn->resid_dropout', 'transformer->h->1->attn->k_proj', 'transformer->h->1->attn->v_proj', 'transformer->h->1->attn->q_proj', 'transformer->h->1->attn->out_proj', 'transformer->h->1->mlp', 'transformer->h->1->mlp->fc_in', 'transformer->h->1->mlp->fc_out', 'transformer->h->1->mlp->act', 'transformer->h->1->mlp->dropout', 'transformer->h->2', 'transformer->h->2->ln_1', 'transformer->h->2->attn', 'transformer->h->2->attn->attn_dropout', 'transformer->h->2->attn->resid_dropout', 'transformer->h->2->attn->k_proj', 'transformer->h->2->attn->v_proj', 'transformer->h->2->attn->q_proj', 'transformer->h->2->attn->out_proj', 'transformer->h->2->mlp', 'transformer->h->2->mlp->fc_in', 'transformer->h->2->mlp->fc_out', 'transformer->h->2->mlp->act', 'transformer->h->2->mlp->dropout', 'transformer->h->3', 'transformer->h->3->ln_1', 'transformer->h->3->attn', 'transformer->h->3->attn->attn_dropout', 'transformer->h->3->attn->resid_dropout', 'transformer->h->3->attn->k_proj', 'transformer->h->3->attn->v_proj', 'transformer->h->3->attn->q_proj', 'transformer->h->3->attn->out_proj', 'transformer->h->3->mlp', 'transformer->h->3->mlp->fc_in', 'transformer->h->3->mlp->fc_out', 'transformer->h->3->mlp->act', 'transformer->h->3->mlp->dropout', 'transformer->h->4', 'transformer->h->4->ln_1', 'transformer->h->4->attn', 'transformer->h->4->attn->attn_dropout', 'transformer->h->4->attn->resid_dropout', 'transformer->h->4->attn->k_proj', 'transformer->h->4->attn->v_proj', 'transformer->h->4->attn->q_proj', 'transformer->h->4->attn->out_proj', 'transformer->h->4->mlp', 'transformer->h->4->mlp->fc_in', 'transformer->h->4->mlp->fc_out', 'transformer->h->4->mlp->act', 'transformer->h->4->mlp->dropout', 'transformer->h->5', 'transformer->h->5->ln_1', 'transformer->h->5->attn', 'transformer->h->5->attn->attn_dropout', 'transformer->h->5->attn->resid_dropout', 'transformer->h->5->attn->k_proj', 'transformer->h->5->attn->v_proj', 'transformer->h->5->attn->q_proj', 'transformer->h->5->attn->out_proj', 'transformer->h->5->mlp', 'transformer->h->5->mlp->fc_in', 'transformer->h->5->mlp->fc_out', 'transformer->h->5->mlp->act', 'transformer->h->5->mlp->dropout', 'transformer->h->6', 'transformer->h->6->ln_1', 'transformer->h->6->attn', 'transformer->h->6->attn->attn_dropout', 'transformer->h->6->attn->resid_dropout', 'transformer->h->6->attn->k_proj', 'transformer->h->6->attn->v_proj', 'transformer->h->6->attn->q_proj', 'transformer->h->6->attn->out_proj', 'transformer->h->6->mlp', 'transformer->h->6->mlp->fc_in', 'transformer->h->6->mlp->fc_out', 'transformer->h->6->mlp->act', 'transformer->h->6->mlp->dropout', 'transformer->h->7', 'transformer->h->7->ln_1', 'transformer->h->7->attn', 'transformer->h->7->attn->attn_dropout', 'transformer->h->7->attn->resid_dropout', 'transformer->h->7->attn->k_proj', 'transformer->h->7->attn->v_proj', 'transformer->h->7->attn->q_proj', 'transformer->h->7->attn->out_proj', 'transformer->h->7->mlp', 'transformer->h->7->mlp->fc_in', 'transformer->h->7->mlp->fc_out', 'transformer->h->7->mlp->act', 'transformer->h->7->mlp->dropout', 'transformer->h->8', 'transformer->h->8->ln_1', 'transformer->h->8->attn', 'transformer->h->8->attn->attn_dropout', 'transformer->h->8->attn->resid_dropout', 'transformer->h->8->attn->k_proj', 'transformer->h->8->attn->v_proj', 'transformer->h->8->attn->q_proj', 'transformer->h->8->attn->out_proj', 'transformer->h->8->mlp', 'transformer->h->8->mlp->fc_in', 'transformer->h->8->mlp->fc_out', 'transformer->h->8->mlp->act', 'transformer->h->8->mlp->dropout', 'transformer->h->9', 'transformer->h->9->ln_1', 'transformer->h->9->attn', 'transformer->h->9->attn->attn_dropout', 'transformer->h->9->attn->resid_dropout', 'transformer->h->9->attn->k_proj', 'transformer->h->9->attn->v_proj', 'transformer->h->9->attn->q_proj', 'transformer->h->9->attn->out_proj', 'transformer->h->9->mlp', 'transformer->h->9->mlp->fc_in', 'transformer->h->9->mlp->fc_out', 'transformer->h->9->mlp->act', 'transformer->h->9->mlp->dropout', 'transformer->h->10', 'transformer->h->10->ln_1', 'transformer->h->10->attn', 'transformer->h->10->attn->attn_dropout', 'transformer->h->10->attn->resid_dropout', 'transformer->h->10->attn->k_proj', 'transformer->h->10->attn->v_proj', 'transformer->h->10->attn->q_proj', 'transformer->h->10->attn->out_proj', 'transformer->h->10->mlp', 'transformer->h->10->mlp->fc_in', 'transformer->h->10->mlp->fc_out', 'transformer->h->10->mlp->act', 'transformer->h->10->mlp->dropout', 'transformer->h->11', 'transformer->h->11->ln_1', 'transformer->h->11->attn', 'transformer->h->11->attn->attn_dropout', 'transformer->h->11->attn->resid_dropout', 'transformer->h->11->attn->k_proj', 'transformer->h->11->attn->v_proj', 'transformer->h->11->attn->q_proj', 'transformer->h->11->attn->out_proj', 'transformer->h->11->mlp', 'transformer->h->11->mlp->fc_in', 'transformer->h->11->mlp->fc_out', 'transformer->h->11->mlp->act', 'transformer->h->11->mlp->dropout', 'transformer->h->12', 'transformer->h->12->ln_1', 'transformer->h->12->attn', 'transformer->h->12->attn->attn_dropout', 'transformer->h->12->attn->resid_dropout', 'transformer->h->12->attn->k_proj', 'transformer->h->12->attn->v_proj', 'transformer->h->12->attn->q_proj', 'transformer->h->12->attn->out_proj', 'transformer->h->12->mlp', 'transformer->h->12->mlp->fc_in', 'transformer->h->12->mlp->fc_out', 'transformer->h->12->mlp->act', 'transformer->h->12->mlp->dropout', 'transformer->h->13', 'transformer->h->13->ln_1', 'transformer->h->13->attn', 'transformer->h->13->attn->attn_dropout', 'transformer->h->13->attn->resid_dropout', 'transformer->h->13->attn->k_proj', 'transformer->h->13->attn->v_proj', 'transformer->h->13->attn->q_proj', 'transformer->h->13->attn->out_proj', 'transformer->h->13->mlp', 'transformer->h->13->mlp->fc_in', 'transformer->h->13->mlp->fc_out', 'transformer->h->13->mlp->act', 'transformer->h->13->mlp->dropout', 'transformer->h->14', 'transformer->h->14->ln_1', 'transformer->h->14->attn', 'transformer->h->14->attn->attn_dropout', 'transformer->h->14->attn->resid_dropout', 'transformer->h->14->attn->k_proj', 'transformer->h->14->attn->v_proj', 'transformer->h->14->attn->q_proj', 'transformer->h->14->attn->out_proj', 'transformer->h->14->mlp', 'transformer->h->14->mlp->fc_in', 'transformer->h->14->mlp->fc_out', 'transformer->h->14->mlp->act', 'transformer->h->14->mlp->dropout', 'transformer->h->15', 'transformer->h->15->ln_1', 'transformer->h->15->attn', 'transformer->h->15->attn->attn_dropout', 'transformer->h->15->attn->resid_dropout', 'transformer->h->15->attn->k_proj', 'transformer->h->15->attn->v_proj', 'transformer->h->15->attn->q_proj', 'transformer->h->15->attn->out_proj', 'transformer->h->15->mlp', 'transformer->h->15->mlp->fc_in', 'transformer->h->15->mlp->fc_out', 'transformer->h->15->mlp->act', 'transformer->h->15->mlp->dropout', 'transformer->h->16', 'transformer->h->16->ln_1', 'transformer->h->16->attn', 'transformer->h->16->attn->attn_dropout', 'transformer->h->16->attn->resid_dropout', 'transformer->h->16->attn->k_proj', 'transformer->h->16->attn->v_proj', 'transformer->h->16->attn->q_proj', 'transformer->h->16->attn->out_proj', 'transformer->h->16->mlp', 'transformer->h->16->mlp->fc_in', 'transformer->h->16->mlp->fc_out', 'transformer->h->16->mlp->act', 'transformer->h->16->mlp->dropout', 'transformer->h->17', 'transformer->h->17->ln_1', 'transformer->h->17->attn', 'transformer->h->17->attn->attn_dropout', 'transformer->h->17->attn->resid_dropout', 'transformer->h->17->attn->k_proj', 'transformer->h->17->attn->v_proj', 'transformer->h->17->attn->q_proj', 'transformer->h->17->attn->out_proj', 'transformer->h->17->mlp', 'transformer->h->17->mlp->fc_in', 'transformer->h->17->mlp->fc_out', 'transformer->h->17->mlp->act', 'transformer->h->17->mlp->dropout', 'transformer->h->18', 'transformer->h->18->ln_1', 'transformer->h->18->attn', 'transformer->h->18->attn->attn_dropout', 'transformer->h->18->attn->resid_dropout', 'transformer->h->18->attn->k_proj', 'transformer->h->18->attn->v_proj', 'transformer->h->18->attn->q_proj', 'transformer->h->18->attn->out_proj', 'transformer->h->18->mlp', 'transformer->h->18->mlp->fc_in', 'transformer->h->18->mlp->fc_out', 'transformer->h->18->mlp->act', 'transformer->h->18->mlp->dropout', 'transformer->h->19', 'transformer->h->19->ln_1', 'transformer->h->19->attn', 'transformer->h->19->attn->attn_dropout', 'transformer->h->19->attn->resid_dropout', 'transformer->h->19->attn->k_proj', 'transformer->h->19->attn->v_proj', 'transformer->h->19->attn->q_proj', 'transformer->h->19->attn->out_proj', 'transformer->h->19->mlp', 'transformer->h->19->mlp->fc_in', 'transformer->h->19->mlp->fc_out', 'transformer->h->19->mlp->act', 'transformer->h->19->mlp->dropout', 'transformer->h->20', 'transformer->h->20->ln_1', 'transformer->h->20->attn', 'transformer->h->20->attn->attn_dropout', 'transformer->h->20->attn->resid_dropout', 'transformer->h->20->attn->k_proj', 'transformer->h->20->attn->v_proj', 'transformer->h->20->attn->q_proj', 'transformer->h->20->attn->out_proj', 'transformer->h->20->mlp', 'transformer->h->20->mlp->fc_in', 'transformer->h->20->mlp->fc_out', 'transformer->h->20->mlp->act', 'transformer->h->20->mlp->dropout', 'transformer->h->21', 'transformer->h->21->ln_1', 'transformer->h->21->attn', 'transformer->h->21->attn->attn_dropout', 'transformer->h->21->attn->resid_dropout', 'transformer->h->21->attn->k_proj', 'transformer->h->21->attn->v_proj', 'transformer->h->21->attn->q_proj', 'transformer->h->21->attn->out_proj', 'transformer->h->21->mlp', 'transformer->h->21->mlp->fc_in', 'transformer->h->21->mlp->fc_out', 'transformer->h->21->mlp->act', 'transformer->h->21->mlp->dropout', 'transformer->h->22', 'transformer->h->22->ln_1', 'transformer->h->22->attn', 'transformer->h->22->attn->attn_dropout', 'transformer->h->22->attn->resid_dropout', 'transformer->h->22->attn->k_proj', 'transformer->h->22->attn->v_proj', 'transformer->h->22->attn->q_proj', 'transformer->h->22->attn->out_proj', 'transformer->h->22->mlp', 'transformer->h->22->mlp->fc_in', 'transformer->h->22->mlp->fc_out', 'transformer->h->22->mlp->act', 'transformer->h->22->mlp->dropout', 'transformer->h->23', 'transformer->h->23->ln_1', 'transformer->h->23->attn', 'transformer->h->23->attn->attn_dropout', 'transformer->h->23->attn->resid_dropout', 'transformer->h->23->attn->k_proj', 'transformer->h->23->attn->v_proj', 'transformer->h->23->attn->q_proj', 'transformer->h->23->attn->out_proj', 'transformer->h->23->mlp', 'transformer->h->23->mlp->fc_in', 'transformer->h->23->mlp->fc_out', 'transformer->h->23->mlp->act', 'transformer->h->23->mlp->dropout', 'transformer->h->24', 'transformer->h->24->ln_1', 'transformer->h->24->attn', 'transformer->h->24->attn->attn_dropout', 'transformer->h->24->attn->resid_dropout', 'transformer->h->24->attn->k_proj', 'transformer->h->24->attn->v_proj', 'transformer->h->24->attn->q_proj', 'transformer->h->24->attn->out_proj', 'transformer->h->24->mlp', 'transformer->h->24->mlp->fc_in', 'transformer->h->24->mlp->fc_out', 'transformer->h->24->mlp->act', 'transformer->h->24->mlp->dropout', 'transformer->h->25', 'transformer->h->25->ln_1', 'transformer->h->25->attn', 'transformer->h->25->attn->attn_dropout', 'transformer->h->25->attn->resid_dropout', 'transformer->h->25->attn->k_proj', 'transformer->h->25->attn->v_proj', 'transformer->h->25->attn->q_proj', 'transformer->h->25->attn->out_proj', 'transformer->h->25->mlp', 'transformer->h->25->mlp->fc_in', 'transformer->h->25->mlp->fc_out', 'transformer->h->25->mlp->act', 'transformer->h->25->mlp->dropout', 'transformer->h->26', 'transformer->h->26->ln_1', 'transformer->h->26->attn', 'transformer->h->26->attn->attn_dropout', 'transformer->h->26->attn->resid_dropout', 'transformer->h->26->attn->k_proj', 'transformer->h->26->attn->v_proj', 'transformer->h->26->attn->q_proj', 'transformer->h->26->attn->out_proj', 'transformer->h->26->mlp', 'transformer->h->26->mlp->fc_in', 'transformer->h->26->mlp->fc_out', 'transformer->h->26->mlp->act', 'transformer->h->26->mlp->dropout', 'transformer->h->27', 'transformer->h->27->ln_1', 'transformer->h->27->attn', 'transformer->h->27->attn->attn_dropout', 'transformer->h->27->attn->resid_dropout', 'transformer->h->27->attn->k_proj', 'transformer->h->27->attn->v_proj', 'transformer->h->27->attn->q_proj', 'transformer->h->27->attn->out_proj', 'transformer->h->27->mlp', 'transformer->h->27->mlp->fc_in', 'transformer->h->27->mlp->fc_out', 'transformer->h->27->mlp->act', 'transformer->h->27->mlp->dropout', 'transformer->ln_f', 'lm_head']\n"
     ]
    }
   ],
   "source": [
    "# layer names\n",
    "print(list(model.layers.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "f = open(\"../../data/known_1000.json\")\n",
    "prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vinson Massif', 'Vinson Massif is located in the continent of Antarctica')\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "texts = [(prompts[i]['subject'], prompts[i]['prompt'] + ' ' + prompts[i]['attribute']) for i in range(len(prompts))]\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens:  6 text:  iPod Touch is developed by Apple\n"
     ]
    }
   ],
   "source": [
    "### Batching requires inputs to be of the same size so we put each prompt in \n",
    "### a bucket depending on the number of tokens it contains.\n",
    "\n",
    "inputs = [[] for z in range(30)] ## use 30 as an upper bound on the number of tokens\n",
    "for subject, text in random.sample(texts, 1000):\n",
    "    encoded_text, target_id = utility.prepare_input(text, tokenizer, device)\n",
    "    num_tokens = encoded_text['input_ids'].shape[1]\n",
    "    \n",
    "    print(\"num tokens: \", num_tokens, \"text: \", text)\n",
    "    \n",
    "    # get tokens\n",
    "    input_ids = encoded_text['input_ids'][0]\n",
    "    tkens = [tokenizer.decode(input_ids[i]) for i in range(len(input_ids))]\n",
    "    \n",
    "    # get position of subject tokens\n",
    "    subject_positions = utility.get_subject_positions(tkens, subject)\n",
    "    indx_str = str(subject_positions[0]) + \":\" + str(subject_positions[-1]+1)\n",
    "    \n",
    "    # add star to subject tokens to later do analysis\n",
    "    start, end = subject_positions[0], subject_positions[1]\n",
    "    while start < end:\n",
    "        tkens[start] = tkens[start] + \"*\"\n",
    "        start += 1\n",
    "    \n",
    "    # save uncorrupted states for patching\n",
    "    num_layers = get_num_layers(model, layer_key_prefix='transformer->h')\n",
    "    output_hooks_names = [f'transformer->h->{layer}' for layer in range(num_layers)]\n",
    "    output_hooks = [Hook(name, utility.save_output, name) for name in output_hooks_names]\n",
    "    model(**encoded_text, hooks=output_hooks)\n",
    "    \n",
    "    hidden_states = [model.save_ctx[f'transformer->h->{layer}']['output'][0].detach() for layer in range(num_layers)] # need detach\n",
    "    \n",
    "    # get probability with uncorrupted subject\n",
    "    p = model(**encoded_text, hooks=[])['logits'].softmax(dim=-1)[0,-1,target_id].item()\n",
    "    \n",
    "    NUM_RUNS = 10 # ROME runs each input 10 times\n",
    "    for i in range(NUM_RUNS):\n",
    "        # add noise hook\n",
    "        seed = np.random.randint(100, size=1)[0]\n",
    "        noise_hook = Hook(\n",
    "            layer_name='transformer->wte',\n",
    "            func=hooks.additive_output_noise(indices=indx_str, mean=0, std=0.1, seed=seed),\n",
    "            key='embedding_noise',\n",
    "        )\n",
    "    \n",
    "        # get probability with corrupted subject\n",
    "        p_star = model(**encoded_text, hooks=[noise_hook])['logits'].softmax(dim=-1)[0,-1,target_id].item()\n",
    "        \n",
    "        inputs[num_tokens].append((num_tokens, num_layers, encoded_text, hidden_states, tkens, text, target_id, indx_str, seed, p, p_star))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens:  6 number of prompts:  1\n"
     ]
    }
   ],
   "source": [
    "# statistics on number of prompts for each length tokens\n",
    "for i in range(len(inputs)):\n",
    "    length = len(inputs[i])\n",
    "    if length != 0:\n",
    "        print(\"num tokens: \", i, \"number of prompts: \", int(length/NUM_RUNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove input buckets with 0 prompts\n",
    "inputs = [inp for inp in inputs if inp != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with longest prompts first :)\n",
    "inputs.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts in this bucket: 10.\n",
      "This batch has 10 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e181a2a37824ff18eac938402616c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "results = {}\n",
    "batch_schedule = {i : int(1250/i) for i in range(1, 30)}\n",
    "for inp in inputs:\n",
    "    print(f\"Number of prompts in this bucket: {len(inp)}.\")\n",
    "    \n",
    "    batch_size = batch_schedule[inp[0][0]]\n",
    "    batched_data = DataLoader(inp, batch_size=batch_size)\n",
    "    for data in batched_data:\n",
    "        curr_batch_size = data[0].size()[0]\n",
    "        print(\"This batch has\", curr_batch_size, \"examples.\")\n",
    "        \n",
    "        num_tokens = data[0][0].item()\n",
    "        num_layers = data[1][0].item()\n",
    "        encoded_text = data[2]\n",
    "        hidden_states = data[3]\n",
    "        tkens = [[t[i] for t in data[4]] for i in range(curr_batch_size)]\n",
    "        text = [t for t in data[5]]\n",
    "        target_ids = [t.item() for t in data[6]]\n",
    "        indx_strs = [s for s in data[7]]\n",
    "        seeds = [seed.item() for seed in data[8]]\n",
    "        p = [p.item() for p in data[9]]\n",
    "        p_stars = [p_s.item() for p_s in data[10]]\n",
    "\n",
    "        noise_hooks = [] # can probably be batched [will take a look later on]\n",
    "        for i in range(curr_batch_size):\n",
    "            noise_hook = Hook(\n",
    "                layer_name='transformer->wte',\n",
    "                func=hooks.additive_output_noise(indices=indx_strs[i], mean=0, std=0.1, index=i, seed=seeds[i]),\n",
    "                key='embedding_noise',\n",
    "            )\n",
    "            noise_hooks.append(noise_hook)\n",
    "            \n",
    "        for layer in tqdm(range(num_layers)):\n",
    "            for position in range(num_tokens):\n",
    "                hook = Hook(\n",
    "                    layer_name=f'transformer->h->{layer}',\n",
    "                    func= hooks.hidden_patch_hook_fn(layer, position, hidden_states),\n",
    "                    key=f'patch_{layer}_pos{position}'\n",
    "                )\n",
    "                output = model(**encoded_text, hooks=noise_hooks+[hook])\n",
    "                for i in range(curr_batch_size):\n",
    "                    if text[i] not in results:\n",
    "                        results[text[i]] = (torch.zeros((num_tokens, num_layers)), (tkens[i], num_layers, sum(p) / len(p), sum(p_stars) / len(p_stars)))\n",
    "                    results[text[i]][0][position, layer] += torch.softmax(output[\"logits\"][i][0,-1,:], 0)[target_ids[i]].item() - p_stars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to divide by NUM_RUNS\n",
    "res = []\n",
    "for text, value in results.items():\n",
    "    result, access = value\n",
    "    result = result / NUM_RUNS\n",
    "    res.append((result, access))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(res, \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(\"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
